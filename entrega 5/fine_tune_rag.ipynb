{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# FINE TUNING DE UN MODELO LLM"
      ],
      "metadata": {
        "id": "0N0q7TxnPKLk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* \"Train the retriever to improve what is retrieved — better inputs to the generator.\"\n",
        "\n",
        "* \"Train the generator to improve how it responds — better outputs based on those inputs.\""
      ],
      "metadata": {
        "id": "lMeXDyJNPYWp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Preparar dataset de QA (preguntas y respuestas)"
      ],
      "metadata": {
        "id": "D9njdVE2PXpx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "# configuration files\n",
        "\n",
        "MANUAL_PATH = \"manual_usuario_mi_coto_clean.txt\"\n",
        "QUESTIONS_PATH = \"questions.json\"\n",
        "OUTPUT_PATH = \"qa_pairs.jsonl\"\n",
        "\n",
        "\n",
        "# load data\n",
        "with open(QUESTIONS_PATH, \"r\") as f:\n",
        "    questions = json.load(f)  # List of questions"
      ],
      "metadata": {
        "id": "Yks8Rw3GPS5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Generar pares de preguntas y respuestas con las cuales será entrenado el Retriever, esto ayudará a que identifique con mayor facilidad las respuestas en los diferentes chunks"
      ],
      "metadata": {
        "id": "3mc7LFGlPiK8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generar pares de preguntas y respuestas\n",
        "\n",
        "qa_pairs = []\n",
        "for question in questions:\n",
        "    result = qa_chain.run(question)\n",
        "    qa_pairs.append({\"question\": question, \"answer\": result})\n",
        "\n",
        "# guardar pares de preguntas y respuestas\n",
        "\n",
        "with open(OUTPUT_PATH, \"w\", encoding=\"MacRoman\") as f:\n",
        "    for pair in qa_pairs:\n",
        "        json.dump(pair, f)\n",
        "        f.write(\"\\n\")\n",
        "\n",
        "qa_pairs"
      ],
      "metadata": {
        "id": "T0iPH2DnPipI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Fine-tune Retriever: en esta parte se entrena el retriever con las diferentes formas de preguntas / responder a los usuarios\n"
      ],
      "metadata": {
        "id": "9TIqbrpLPoNd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "from sentence_transformers import SentenceTransformer, losses, InputExample\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "train_examples = [InputExample(texts=[question, context]) for question, context in qa_pairs]\n",
        "model_retriever = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\n",
        "train_loss = losses.MultipleNegativesRankingLoss(model_retriever)\n",
        "model_retriever.fit(train_objectives=[(train_dataloader, train_loss)], epochs=3)\n",
        "model_retriever.save(\"models/custom_retriever\")"
      ],
      "metadata": {
        "id": "v4-OWH3HPleL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Fine tune Generator"
      ],
      "metadata": {
        "id": "kY5aiVvePtXK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "\n",
        "# Assume qa_pairs is already defined\n",
        "input_texts = [f\"Pregunta: {qa['question']} Contexto: {qa['answer']}\" for qa in qa_pairs]\n",
        "target_texts = [qa['answer'] for qa in qa_pairs]\n",
        "\n",
        "# Load a pre-trained model like T5 or BART for sequence-to-sequence tasks (BERT isn't ideal for generation tasks)\n",
        "model_generator = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
        "\n",
        "# Tokenize the input and target texts\n",
        "inputs = tokenizer(input_texts, padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n",
        "labels = tokenizer(target_texts, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n",
        "\n",
        "# Move to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_generator = model_generator.to(device)\n",
        "\n",
        "# Create a DataLoader for batching\n",
        "dataset = list(zip(inputs[\"input_ids\"], inputs[\"attention_mask\"], labels[\"input_ids\"]))\n",
        "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.AdamW(model_generator.parameters(), lr=5e-5)\n",
        "\n",
        "# Training loop\n",
        "model_generator.train()\n",
        "for epoch in range(3):\n",
        "    for batch in dataloader:\n",
        "        input_ids, attention_mask, label_ids = [b.to(device) for b in batch]\n",
        "\n",
        "        outputs = model_generator(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=label_ids\n",
        "        )\n",
        "\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    print(f\"Epoch {epoch+1} completed\")\n",
        "\n",
        "# Save model and tokenizer\n",
        "output_dir = \"./my_finetuned_model\"\n",
        "model_generator.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n"
      ],
      "metadata": {
        "id": "D_nHWHBDPwNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Loading customized generator"
      ],
      "metadata": {
        "id": "SN-WvnKTP48D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Wrap it in a generation pipeline\n",
        "text_gen_pipeline = pipeline(\n",
        "    \"text2text-generation\",  # For encoder-decoder models\n",
        "    model= model_generator,\n",
        "    tokenizer=tokenizer,\n",
        "    max_length=829,\n",
        "    do_sample=False,\n",
        "    temperature=0.7,\n",
        "    top_p=0.9,\n",
        "    repetition_penalty=1.1\n",
        ")\n",
        "\n",
        "# Wrap pipeline in LangChain LLM\n",
        "fine_tuned_llm = HuggingFacePipeline(pipeline=text_gen_pipeline)"
      ],
      "metadata": {
        "id": "C4kM4HAmP5TL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Loading customized retriever"
      ],
      "metadata": {
        "id": "SAS6AguZP_QN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load trained_retriever\n",
        "model_retriever = SentenceTransformer('models/custom_retriever')\n",
        "\n",
        "# Convert SentenceTransformer into a LangChain embedding model\n",
        "embedding_function = HuggingFaceEmbeddings(model_name='models/custom_retriever')\n",
        "\n",
        "# Prepare LangChain Documents\n",
        "langchain_docs = [Document(page_content=doc.page_content) for doc in docs]  # docs from text_splitter\n",
        "# langchain_docs = docs\n",
        "\n",
        "# Create FAISS vector store\n",
        "vectorstore = FAISS.from_documents(langchain_docs, embedding_function)\n",
        "\n",
        "# This is your retriever\n",
        "fine_tuned_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})"
      ],
      "metadata": {
        "id": "F_4WFJigP8T5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Testing new rag with fine-tuned retriever and fine-tuned generator"
      ],
      "metadata": {
        "id": "jwrH5ONqQDgV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = PromptTemplate.from_template(\n",
        "    \"\"\"Responde en español basándote únicamente en el contexto proporcionado:\n",
        "\n",
        "{context}\n",
        "\n",
        "Pregunta: {question}\n",
        "Respuesta:\"\"\"\n",
        ")\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=fine_tuned_llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever= fine_tuned_retriever,\n",
        "    chain_type_kwargs={\"prompt\": prompt_template},\n",
        "    return_source_documents=True\n",
        ")"
      ],
      "metadata": {
        "id": "FipXbOg4QDqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "questions = [\n",
        "    \"¿Cómo cambio mi contraseña?\",\n",
        "    \"¿Dónde puedo actualizar mi teléfono?\",\n",
        "    \"¿Dónde se sube la constancia fiscal?\",\n",
        "    \"¿Dónde veo mi saldo o mis adeudos?\",\n",
        "    \"¿Cómo informo que ya pagué?\",\n",
        "    \"¿Dónde agrego mis datos fiscales?\",\n",
        "    \"¿Cómo aparto el salón de eventos?\",\n",
        "    \"¿Cómo puedo cancelar una reservación?\",\n",
        "    \"Envié un mensaje y no me han respondido\",\n",
        "    \"¿Dónde veo lo que escribió el administrador?\",\n",
        "    \"¿Dónde están las actas de la asamblea?\",\n",
        "    \"¿Dónde está el reglamento del condominio?\"\n",
        "]\n",
        "\n",
        "for q in questions:\n",
        "    result = qa_chain({\"query\": q})\n",
        "    print(f\"\\nPregunta: {q}\")\n",
        "    print(\"Respuesta:\", result[\"result\"])\n",
        "\n",
        "    # print(\"Contexto usado:\")\n",
        "    # for i, doc in enumerate(result[\"source_documents\"], 1):\n",
        "      # print(f\"  [{i}] {doc.page_content.strip()}\")"
      ],
      "metadata": {
        "id": "Doddtf9lQGCl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}