{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "from langchain.schema import Document\n",
    "from langchain.document_loaders import PyPDFLoader, TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, SentenceTransformersTokenTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings, SentenceTransformerEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.language_models.fake import FakeListLLM  # Importa el LLM falso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuración ---\n",
    "DOCUMENT_PATH = \"./data\"\n",
    "FAISS_INDEX_PATH = \"./my_faiss_index\"\n",
    "EMBEDDINGS_MODEL = \"all-MiniLM-L6-v2\"  # Puedes probar \"sentence-transformers/all-mpnet-base-v2\" o uno específico\n",
    "CHUNK_SIZE = 1000\n",
    "CHUNK_OVERLAP = 100\n",
    "USE_SEMANTIC_CHUNKING = True  # Cambiar a False para usar RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    # 1) Eliminar cabeceras/pies de página comunes\n",
    "    text = re.sub(r\"Página\\s*\\d+\\s*/\\s*\\d+\", \" \", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"Manual de Usuario|Funcionalidad MiCoto\", \" \", text)\n",
    "\n",
    "    # 2) Deshacer guiones al final de renglón: “co-\\nmentario” → “comentario”\n",
    "    text = re.sub(r\"(\\w)-\\s*\\n\\s*(\\w)\", r\"\\1\\2\", text)\n",
    "\n",
    "    # 3) Unir saltos de línea dentro del mismo párrafo\n",
    "    text = re.sub(r\"(?<!\\n)\\n(?!\\n)\", \" \", text)\n",
    "\n",
    "    # 4) Quitar URLs, correos y teléfonos\n",
    "    text = re.sub(r\"\\bhttps?://\\S+\\b\", \" \", text)\n",
    "    text = re.sub(r\"\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w{2,}\\b\", \" \", text)\n",
    "    text = re.sub(r\"\\b\\d{2,4}[-\\s]?\\d{2,4}[-\\s]?\\d{2,4}\\b\", \" \", text)\n",
    "\n",
    "    # 5) Unificar comillas y guiones largos\n",
    "    text = text.replace(\"“\", '\"').replace(\"”\", '\"').replace(\"—\", \"-\")\n",
    "    # 6) Colapsar múltiples espacios y líneas en blanco\n",
    "    text = re.sub(r\"[ \\t]{2,}\", \" \", text)\n",
    "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
    "\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_useful_for_rag(text: str) -> bool:\n",
    "    t = text.strip()\n",
    "    return len(t) > 50 and len(t.split()) >= 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdfs_and_txts(path: str):\n",
    "    docs = []\n",
    "    # PDFs\n",
    "    for pdf_path in glob.glob(os.path.join(path, \"*.pdf\")):\n",
    "        loader = PyPDFLoader(pdf_path)\n",
    "        for d in loader.load():\n",
    "            txt = clean_text(d.page_content)\n",
    "            if is_useful_for_rag(txt):\n",
    "                d.page_content = txt\n",
    "                d.metadata[\"source\"] = os.path.basename(pdf_path)\n",
    "                docs.append(d)\n",
    "\n",
    "    # TXT (opcional)\n",
    "    for txt_path in glob.glob(os.path.join(path, \"*.txt\")):\n",
    "        loader = TextLoader(txt_path, encoding=\"utf-8\")\n",
    "        for d in loader.load():\n",
    "            txt = clean_text(d.page_content)\n",
    "            if is_useful_for_rag(txt):\n",
    "                d.page_content = txt\n",
    "                d.metadata[\"source\"] = os.path.basename(txt_path)\n",
    "                docs.append(d)\n",
    "\n",
    "    # Deduplicación\n",
    "    seen, unique = set(), []\n",
    "    for d in docs:\n",
    "        h = hash(d.page_content)\n",
    "        if h not in seen:\n",
    "            seen.add(h)\n",
    "            unique.append(d)\n",
    "\n",
    "    print(f\"→ Documentos únicos cargados: {len(unique)}\")\n",
    "    return unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rag_index():\n",
    "    # 1) Carga y limpieza\n",
    "    documents = load_pdfs_and_txts(DOCUMENT_PATH)\n",
    "\n",
    "    # 2) Chunking\n",
    "    if USE_SEMANTIC_CHUNKING:\n",
    "        splitter = SentenceTransformersTokenTextSplitter(chunk_overlap=CHUNK_OVERLAP, chunk_size=CHUNK_SIZE)\n",
    "        chunks = splitter.split_documents(documents)\n",
    "        print(f\"→ Se generaron {len(chunks)} chunks (usando Sentence Transformers for chunking).\")\n",
    "    else:\n",
    "        splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
    "        chunks = splitter.split_documents(documents)\n",
    "        print(f\"→ Se generaron {len(chunks)} chunks (usando RecursiveCharacterTextSplitter).\")\n",
    "\n",
    "    # 3) Embeddings\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=EMBEDDINGS_MODEL,\n",
    "        model_kwargs={\"device\": \"cpu\"}\n",
    "    )\n",
    "    print(f\"→ Modelo de embeddings cargado: {EMBEDDINGS_MODEL}\")\n",
    "\n",
    "    # 4) FAISS\n",
    "    vector_store = FAISS.from_documents(chunks, embeddings)\n",
    "    vector_store.save_local(FAISS_INDEX_PATH)\n",
    "    print(f\"✅ Índice FAISS creado en: {FAISS_INDEX_PATH}\")\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_rag(vector_store):\n",
    "    \"\"\"Función simple para probar el RAG localmente.\"\"\"\n",
    "    # Usamos un LLM falso para probar la recuperación\n",
    "    llm = FakeListLLM(responses=[\"Respuesta basada en el contexto.\"])\n",
    "\n",
    "    prompt_template = \"\"\"Eres un asistente útil. Responde la pregunta basándote únicamente en el siguiente contexto.\n",
    "    Si la respuesta no está en el contexto, di \"No tengo suficiente información para responder a eso\". No inventes.\n",
    "\n",
    "    Contexto: {context}\n",
    "\n",
    "    Pregunta: {question}\n",
    "\n",
    "    Respuesta:\"\"\"\n",
    "    QA_CHAIN_PROMPT = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,  # Pasa la instancia del LLM falso\n",
    "        retriever=vector_store.as_retriever(),\n",
    "        chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT},\n",
    "        return_source_documents=True\n",
    "    )\n",
    "\n",
    "    while True:\n",
    "        query = input(\"Pregunta al RAG (o escribe 'salir'): \")\n",
    "        if query.lower() == 'salir':\n",
    "            break\n",
    "\n",
    "        result = qa_chain({\"query\": query})\n",
    "        print(\"\\nRespuesta:\", result[\"result\"])\n",
    "        print(\"\\nFuentes:\")\n",
    "        for doc in result[\"source_documents\"]:\n",
    "            print(f\"- {doc.metadata['source']}: {doc.page_content[:100]}...\") # Mostrar los primeros 100 caracteres\n",
    "        print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 300 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Documentos únicos cargados: 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-11 00:06:26.898823: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/opt/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Se generaron 30 chunks (usando Sentence Transformers for chunking).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wj/697d4ndd5cnd828k3szpyr0c0000gn/T/ipykernel_32661/542114483.py:16: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Modelo de embeddings cargado: all-MiniLM-L6-v2\n",
      "✅ Índice FAISS creado en: ./my_faiss_index\n",
      "\n",
      "--- ¡Índice creado! Ahora puedes probar el RAG localmente: ---\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(DOCUMENT_PATH, exist_ok=True)\n",
    "vector_store = create_rag_index()\n",
    "if vector_store:\n",
    "    print(\"\\n--- ¡Índice creado! Ahora puedes probar el RAG localmente: ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wj/697d4ndd5cnd828k3szpyr0c0000gn/T/ipykernel_32661/3442367660.py:28: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = qa_chain({\"query\": query})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Respuesta: Respuesta basada en el contexto.\n",
      "\n",
      "Fuentes:\n",
      "- Manual de Usuario de la Plataforma _Mi Coto_.pdf: requisitos para recibir notificaciones : 1. para recibir notificaciones, debe estar dado de alta en ...\n",
      "- Manual de Usuario de la Plataforma _Mi Coto_.pdf: manual de usuario de la plataforma \" mi coto \" 1. introduccion ● 1. 1 descripcion general de la plat...\n",
      "- Manual de Usuario de la Plataforma _Mi Coto_.pdf: [UNK] 1. 3. 5 mensaje de bienvenida : un mensaje de bienvenida para el usuario. 2. mi cuenta ● 2. 1 ...\n",
      "- ManualdeUsuario.pdf: paso 2 – proporciona tu nombre de usuario y pasword en nuestro sitio web dentro de nuestra aplicacio...\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_rag(vector_store)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
